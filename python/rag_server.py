import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware # ğŸŒŸ [ì¶”ê°€] CORS ë¯¸ë“¤ì›¨ì–´
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv
from operator import itemgetter

# 1. ì„ë² ë”©ì€ OpenAI ìœ ì§€
from langchain_openai import OpenAIEmbeddings 

# 2. LLMì€ Google Gemini ì‚¬ìš©
from langchain_google_genai import ChatGoogleGenerativeAI 

from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import EnsembleRetriever 
from typing import List, Dict, Any

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("GOOGLE_API_KEY"):
    raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. âœŒï¸ ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì„¤ì • ---
INDEX_NAME_POLICY = "policy-chatbot"      # ê¸°ì¡´ ì •ì±… ë°ì´í„°
INDEX_NAME_JOB = "job-postings-index"     # ì‹ ê·œ ì±„ìš© ê³µê³ 
# ---------------------------------------------


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    # ğŸŒŸ [ëª¨ë¸] Gemini 2.0 Flash Exp
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp", 
        temperature=0.0
    )

    # âš ï¸ [ì„ë² ë”©] OpenAI
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ---------------------------------------------------------
    # ğŸ” 1ë²ˆ ê²€ìƒ‰ê¸°: ì •ì±… ë°ì´í„°
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 1 ì—°ê²° ì¤‘: {INDEX_NAME_POLICY}")
    vectorstore_policy = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_POLICY,
        embedding=embeddings,
        text_key="embedding_text"
    )
    retriever_policy = vectorstore_policy.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} 
    )

    # ---------------------------------------------------------
    # ğŸ” 2ë²ˆ ê²€ìƒ‰ê¸°: ì±„ìš© ê³µê³ 
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 2 ì—°ê²° ì¤‘: {INDEX_NAME_JOB}")
    vectorstore_job = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_JOB,
        embedding=embeddings,
        text_key="context_text"
    )
    retriever_job = vectorstore_job.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} 
    )

    # ---------------------------------------------------------
    # ğŸ¤ ì•™ìƒë¸” ê²€ìƒ‰ê¸° (í†µí•©)
    # ---------------------------------------------------------
    print("ğŸ”— ë‘ ê²€ìƒ‰ê¸°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©(Ensemble)í•©ë‹ˆë‹¤...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[retriever_policy, retriever_job],
        weights=[0.5, 0.5] 
    )

    # ---------------------------------------------------------
    # ğŸŒŸ [í•µì‹¬ ê¸°ëŠ¥] ë©”íƒ€ë°ì´í„° í¬ë§·íŒ… í•¨ìˆ˜
    # Pineconeì˜ 'metadata' í•„ë“œë¥¼ ë„ì§‘ì–´ë‚´ì–´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # ---------------------------------------------------------
    def format_docs_with_metadata(docs):
        formatted_results = []
        for i, doc in enumerate(docs):
            meta = doc.metadata
            content = doc.page_content
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì•ˆì „í•˜ê²Œ ê°’ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ 'ì •ë³´ ì—†ìŒ' ë“±)
            title = meta.get('title') or meta.get('policy_name') or "ì œëª© ì—†ìŒ"
            
            # ì±„ìš© ê³µê³  ê´€ë ¨ í•„ë“œ
            end_date = meta.get('apply_end_date', '')
            method = meta.get('apply_method', '')
            link = meta.get('apply_link', '')
            category = meta.get('job_category', '')
            
            # ì •ì±… ê´€ë ¨ í•„ë“œ (í•„ìš”ì‹œ ì¶”ê°€)
            target = meta.get('target_audience', '')

            # LLMì—ê²Œ ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸ ë¸”ë¡ ì¡°ë¦½
            doc_str = (
                f"--- [ë¬¸ì„œ {i+1}: {title}] ---\n"
                f"ë‚´ìš©: {content}\n"
            )
            
            # ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¼ì¸ ì¶”ê°€ (ê¹”ë”í•˜ê²Œ)
            if end_date: doc_str += f"ë§ˆê°ì¼: {end_date}\n"
            if method: doc_str += f"ì‹ ì²­ë°©ë²•: {method}\n"
            if link: doc_str += f"ë§í¬: {link}\n"
            if category: doc_str += f"ë¶„ì•¼: {category}\n"
            if target: doc_str += f"ëŒ€ìƒ: {target}\n"
            
            formatted_results.append(doc_str)
        
        return "\n\n".join(formatted_results)


    # ğŸŒŸ [í”„ë¡¬í”„íŠ¸] ë©”íƒ€ë°ì´í„° í™œìš© ì§€ì¹¨ ì¶”ê°€
    prompt_template = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ 'ì§€ì—­ ì •ì±…' ë° 'ì±„ìš© ê³µê³ 'ë¥¼ ì•ˆë‚´í•˜ëŠ” ë˜‘ë˜‘í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸, **'Jobs(ì¡ìŠ¤)'**ì…ë‹ˆë‹¤.
    
    [ğŸ§¹ ë°ì´í„° ì •ì œ ë° í•„í„°ë§ ê·œì¹™ (ìµœìš°ì„  ì ìš©)]
    1. **ë¬´ì˜ë¯¸í•œ ì•ŒíŒŒë²³/ê¸°í˜¸ ì ˆëŒ€ ë°œì„¤ ê¸ˆì§€:** ë°ì´í„°ì— "ì‹ ì²­ë°©ë²•: A", "ë¶„ì•¼: B" ì²˜ëŸ¼ ì˜ë¯¸ ì—†ëŠ” ê°’ì´ ìˆë‹¤ë©´ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
    2. **ë©”íƒ€ë°ì´í„° ì ê·¹ í™œìš©:** ì œê³µëœ [ê²€ìƒ‰ ê²°ê³¼]ì—ëŠ” 'ë§ˆê°ì¼', 'ë§í¬', 'ì‹ ì²­ë°©ë²•' ë“±ì˜ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ì´ ì„¸ë¶€ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”.

    [âœ¨ ìƒí™©ë³„ ë‹µë³€ ê°€ì´ë“œ]
    **Case 1. ì¸ì‚¬ ("ì•ˆë…•", "ëˆ„êµ¬ì•¼"):**
    - "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Jobsì…ë‹ˆë‹¤. ì‚¬ìš©ìë‹˜ì˜ ë‚˜ì´ì™€ ì‚¬ëŠ” ê³³ì„ ë¶„ì„í•´ì„œ ë”± ë§ëŠ” ì •ì±…ê³¼ ì¼ìë¦¬ë¥¼ ì°¾ì•„ë“œë¦¬ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤." (ê²€ìƒ‰ ê²°ê³¼ ì–¸ê¸‰ X)
    
    **Case 2. ì •ë³´ ìš”ì²­:**
    - ìê¸°ì†Œê°œ ìƒëµ.
    - ì˜ˆ: "ë„¤, (ì§€ì—­)ì˜ (ë‚˜ì´)ì„¸ ì²­ë…„ì´ ì§€ì› ê°€ëŠ¥í•œ (ì œëª©)ì…ë‹ˆë‹¤. ë§ˆê°ì¼ì€ (ë‚ ì§œ)ê¹Œì§€ì´ë©°, (ë°©ë²•)ìœ¼ë¡œ ì‹ ì²­í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    - **ë§í¬ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì œê³µí•˜ì„¸ìš”.**

    [ğŸ” ìƒì„¸ ì •ë³´ ë‹µë³€ ê·œì¹™]
    - ì‚¬ìš©ìê°€ "êµ¬ì²´ì ìœ¼ë¡œ?"ë¼ê³  ë¬¼ìœ¼ë©´, ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•˜ë˜, ì—†ìœ¼ë©´ ì†”ì§íˆ ì—†ë‹¤ê³  ë§í•˜ì„¸ìš”.

    [ğŸ”µ ë§ì¶¤í˜• ë§¤ì¹­ ì§€ì¹¨]
    - [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]ì™€ ë¹„êµí•˜ì—¬ ì í•©ì„±ì„ íŒë‹¨í•˜ì„¸ìš”.

    [ğŸš« í˜•ì‹ ì œí•œ]
    - ë§ˆí¬ë‹¤ìš´(Markdown), **ë³¼ë“œì²´** ì‚¬ìš© ê¸ˆì§€. ì¤„ê¸€ë¡œë§Œ ì‘ì„±.

    ---
    [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
    {user_context_prompt}

    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    [ì •ì±… ë° ì±„ìš© ë°ì´í„° (ê²€ìƒ‰ ê²°ê³¼)]
    {context}

    [ì§ˆë¬¸]
    {question}

    [Jobsì˜ ë‹µë³€]
    """
    
    PROMPT = PromptTemplate.from_template(prompt_template)

    # ğŸŒŸ [LCEL ì²´ì¸ êµ¬ì„±] (ê¸°ì¡´ ConversationalRetrievalChain ëŒ€ì²´)
    # 1. ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ -> 2. ê²€ìƒ‰ê¸°(retriever)ê°€ ë¬¸ì„œë¥¼ ì°¾ê³  -> 
    # 3. format_docs_with_metadataê°€ ë©”íƒ€ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ -> 4. í”„ë¡¬í”„íŠ¸ -> 5. LLM
    rag_chain = (
        {
            "context": itemgetter("question") | ensemble_retriever | format_docs_with_metadata,
            "question": itemgetter("question"),
            "chat_history": itemgetter("chat_history"),
            "user_context_prompt": itemgetter("user_context_prompt"),
        }
        | PROMPT
        | llm
        | StrOutputParser()
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (LCEL ë°©ì‹ + ë©”íƒ€ë°ì´í„° ì—°ë™).")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤(.env), Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

# ğŸŒŸ [ì¶”ê°€] CORS ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
# ëª¨ë“  ë„ë©”ì¸(origins=["*"])ì—ì„œì˜ ì ‘ê·¼ì„ í—ˆìš©í•©ë‹ˆë‹¤.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []
    user_profile: Dict[str, Any] = {}

class ChatResponse(BaseModel):
    answer: str
    source: str | None = None

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    try:
        user_message = request.message
        chat_history_list = request.history
        user_profile = request.user_profile 
        
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")

        # ì‚¬ìš©ì ì •ë³´ í¬ë§¤íŒ…
        user_context_str = ""
        user_keywords = "" 
        
        if user_profile and (user_profile.get("age") or user_profile.get("region")):
            info_list = []
            if user_profile.get("age") and user_profile.get("age") != "ì•Œ ìˆ˜ ì—†ìŒ":
                age_val = user_profile['age']
                info_list.append(f"- ë‚˜ì´: {age_val}")
                user_keywords += f" {age_val}" 
            if user_profile.get("region") and user_profile.get("region") != "ì•Œ ìˆ˜ ì—†ìŒ":
                region_val = user_profile['region']
                info_list.append(f"- ê±°ì£¼ì§€: {region_val}")
                user_keywords += f" {region_val}" 
            
            if info_list:
                user_context_str = "\n".join(info_list)
            else:
                user_context_str = "(ì‚¬ìš©ì ì •ë³´ ì—†ìŒ)"
        else:
            user_context_str = "(ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ë˜ëŠ” ì •ë³´ ì—†ìŒ)"

        # ëŒ€í™” ê¸°ë¡ í¬ë§¤íŒ… (Stringìœ¼ë¡œ ë³€í™˜)
        formatted_history_str = ""
        user_msg = None
        for turn in chat_history_list:
            if turn.get("sender") == "user":
                user_msg = turn.get("text", "")
            elif turn.get("sender") == "bot" and user_msg is not None:
                formatted_history_str += f"User: {user_msg}\nBot: {turn.get('text', '')}\n"
                user_msg = None 

        # ğŸŒŸ ê²€ìƒ‰ì–´ ë³´ì • (Query Augmentation)
        search_query = user_message
        if "ë‚˜" in user_message or "ë‚´" in user_message or "ì¡°ê±´" in user_message or "ì¶”ì²œ" in user_message:
            search_query += f" {user_keywords}"
            print(f"ğŸ” ë³´ì •ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

        # ğŸŒŸ [LCEL í˜¸ì¶œ] invoke ì‚¬ìš©
        # ì´ì œ chain ë‚´ë¶€ì—ì„œ ê²€ìƒ‰(Retriever)ê³¼ í¬ë§·íŒ…ì´ ìë™ìœ¼ë¡œ ì¼ì–´ë‚©ë‹ˆë‹¤.
        bot_reply = rag_chain.invoke({
            "question": search_query, 
            "chat_history": formatted_history_str,
            "user_context_prompt": user_context_str 
        })
        
        # ì¶œì²˜ í‘œì‹œ ë¡œì§
        source_doc = "ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜"

        print(f"Gemini ë‹µë³€: {bot_reply}")

        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# â€” 5. API ì„œë²„ ì‹¤í–‰ â€”
if __name__ == "__main__":
    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)