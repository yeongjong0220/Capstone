import os
from fastapi import FastAPI
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸ 111
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# ğŸš¨ğŸš¨ğŸš¨ ì´ ë¶€ë¶„ì´ ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë§ê²Œ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
from langchain_pinecone import Pinecone as PineconeVectorStore
# from langchain_pinecone import PineconeVectorStore <-- (ìˆ˜ì • ì „)

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

# .env íŒŒì¼ì— í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. (âš ï¸ ì¤‘ìš”) ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ˜ì •í•  ë¶€ë¶„ ---
# Pineconeì—ì„œ ë¯¸ë¦¬ ìƒì„±í•´ ë‘” "ì¸ë±ìŠ¤ ì´ë¦„"ì„ ì…ë ¥í•˜ì„¸ìš”
# (ë°ì´í„°ê°€ ì´ë¯¸ ì—…ë¡œë“œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)
PINECONE_INDEX_NAME = "policy-chatbot"
# ---------------------------------------------

if PINECONE_INDEX_NAME == "your-pinecone-index-name-here":
    raise ValueError("PINECONE_INDEX_NAMEì„ rag_server.py ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    # 1. LLM (ì–¸ì–´ ëª¨ë¸, ì˜ˆ: GPT-3.5)
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.0 # ë‹µë³€ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ 0.0ìœ¼ë¡œ ì„¤ì •
    )

    # 2. Embedding Model (í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # 3. Vector Store (Pinecone ì¸ë±ìŠ¤ì— ì—°ê²°)
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings
    )

    # 4. Retriever (ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰)
    retriever = vectorstore.as_retriever(
        search_type="similarity", # ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        search_kwargs={'k': 3}  # ìƒìœ„ 3ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜´
    )

    # 5. Prompt Template (LLMì—ê²Œ ë³´ë‚¼ ì§€ì‹œë¬¸ ì–‘ì‹)
    # (ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ ì±—ë´‡ì˜ ë§íˆ¬ë‚˜ ì—­í• ì„ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤)
    prompt_template = """
    ë‹¹ì‹ ì€ 'ì§€ì—­ ì •ì±…' ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.
    ë°˜ë“œì‹œ ì•„ë˜ì— ì œê³µëœ [ì°¸ê³  ìë£Œ]ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
    [ì°¸ê³  ìë£Œ]ì— ì—†ëŠ” ë‚´ìš©ì€ "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    [ì°¸ê³  ìë£Œ]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ë‹µë³€]
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # 6. RAG Chain (ëª¨ë“  êµ¬ì„± ìš”ì†Œë¥¼ í•˜ë‚˜ë¡œ ë¬¶ê¸°)
    # ì´ ì²´ì¸ì´ 1)ì§ˆë¬¸ë°›ê¸° 2)ë¬¸ì„œê²€ìƒ‰ 3)í”„ë¡¬í”„íŠ¸ì¡°í•© 4)LLMë‹µë³€ìƒì„± ì„ ëª¨ë‘ ì²˜ë¦¬
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", # 'stuff'ëŠ” ì°¾ì€ ë¬¸ì„œë¥¼ ëª¨ë‘ contextì— ë„£ëŠ” ë°©ì‹
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True # (ì„ íƒ) ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì„œë¥¼ í•¨ê»˜ ë°˜í™˜
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ.")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤, Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

# Node.jsë¡œë¶€í„° ë°›ì„ ë°ì´í„° ëª¨ë¸
class ChatRequest(BaseModel):
    message: str

# Node.jsì—ê²Œ ë³´ë‚¼ ë°ì´í„° ëª¨ë¸
class ChatResponse(BaseModel):
    answer: str
    source: str | None = None # (ì„ íƒ) ë‹µë³€ì˜ ì¶œì²˜

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    """
    Node.js ë°±ì—”ë“œë¡œë¶€í„° ì§ˆë¬¸ì„ ë°›ì•„ RAG ì±—ë´‡ì„ ì‹¤í–‰í•˜ê³  ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        user_message = request.message
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")

        # [ì‹¤ì œ RAG ì‹¤í–‰]
        response = qa_chain.invoke(user_message)
        
        bot_reply = response['result']
        
        # (ì„ íƒ) ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì„œ ì°¾ê¸°
        source_doc = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        if response.get('source_documents'):
            # ì²« ë²ˆì§¸ ê·¼ê±° ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°(ì˜ˆ: íŒŒì¼ëª…)ë¥¼ ê°€ì ¸ì˜´
            source_doc = response['source_documents'][0].metadata.get('source', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')

        print(f"LLMì´ ìƒì„±í•œ ë‹µë³€: {bot_reply}")
        print(f"ë‹µë³€ ê·¼ê±°: {source_doc}")

        # Node.jsì—ê²Œ JSON í˜•íƒœë¡œ ë‹µë³€ ë°˜í™˜
        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, Python RAG ì„œë²„ì—ì„œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# --- 5. API ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)