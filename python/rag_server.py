import os
from fastapi import FastAPI
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv

# LangChain ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore

# â­ï¸ ìˆ˜ì •ë¨ 1/7: RetrievalQA ëŒ€ì‹  ConversationalRetrievalChain ì„í¬íŠ¸
from langchain.chains import ConversationalRetrievalChain 
from langchain.prompts import PromptTemplate
from typing import List, Dict, Any

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

# .env íŒŒì¼ì— í‚¤ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. (âš ï¸ ì¤‘ìš”) ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ˜ì •í•  ë¶€ë¶„ ---
PINECONE_INDEX_NAME = "policy-chatbot"
# ---------------------------------------------

if PINECONE_INDEX_NAME == "your-pinecone-index-name-here":
    raise ValueError("PINECONE_INDEX_NAMEì„ rag_server.py ì½”ë“œ ë‚´ì—ì„œ ì§ì ‘ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    # 1. LLM (ì–¸ì–´ ëª¨ë¸, ì˜ˆ: GPT-3.5)
    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.0
    )

    # 2. Embedding Model (í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # 3. Vector Store (Pinecone ì¸ë±ìŠ¤ì— ì—°ê²°)
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=PINECONE_INDEX_NAME,
        embedding=embeddings,
        text_key="embedding_text"
    )

    # 4. Retriever (ë²¡í„° ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰)
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3}
    )

    # 5. Prompt Template (ì´ì „ê³¼ ë™ì¼, {chat_history} ë³€ìˆ˜ í¬í•¨)
# 5. Prompt Template (LLMì—ê²Œ ë³´ë‚¼ ì§€ì‹œë¬¸ ì–‘ì‹)
    prompt_template = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ 'ì§€ì—­ ì •ì±…'ì„ ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.
    í•­ìƒ ì‚¬ìš©ìì˜ ê´€ì ì—ì„œ ìƒê°í•˜ë©°, ëª…í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë§íˆ¬ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    [ë‹µë³€ ìƒì„± 7ì›ì¹™]
    1.  **ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë§íˆ¬:** í•­ìƒ ìƒëƒ¥í•˜ê³  ì¹œì ˆí•œ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë©°, ì •ì±… ë‚´ìš©ì„ ì „ë‹¬í•  ë•ŒëŠ” **ê°ê´€ì ì´ê³  ê¶Œìœ„ ìˆëŠ”(authoritative)** ìš©ì–´ë¥¼ ì‚¬ìš©í•´ ì‹ ë¢°ê°ì„ ì£¼ì„¸ìš”.
    2.  **ê°€ë…ì„± ìˆëŠ” í˜•ì‹:** ë‹µë³€ì´ ê¸¸ì–´ì§ˆ ê²½ìš°, **ì¤„ë°”ê¿ˆ**, **ê¸€ë¨¸ë¦¬ ê¸°í˜¸(â€¢)**, **ë²ˆí˜¸ ë§¤ê¸°ê¸°**ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•´ ì •ì±…ë³„ë¡œ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.
    
    3.  **(ğŸš¨ì •ë³´ ì§ì ‘ ì œì‹œ ë° ì™„ì „ì„±):**
        * **ì ˆëŒ€ë¡œ ë‹µë³€ ë‚´ìš©ì— ë‚´ë¶€ ìš©ì–´ì¸ '[ì •ì±… ë°ì´í„°]', '[ì°¸ê³  ìë£Œ]', ë˜ëŠ” 'ìì„¸í•œ ë‚´ìš©ì€ ìë£Œë¥¼ í™•ì¸í•˜ì„¸ìš”'ì™€ ê°™ì€ íšŒí”¼ì„± ë¬¸êµ¬ë¥¼ ì–¸ê¸‰í•˜ê±°ë‚˜ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.**
        * **[ì •ì±… ë°ì´í„°]ì— í¬í•¨ëœ 'ì •ì±…ëª…', 'ëŒ€ìƒ', 'ëª©ì ', 'ì£¼ìš”ë‚´ìš©', 'ì‹ ì²­ë°©ë²•', 'ë¬¸ì˜ì²˜' ë“±**ì˜ **ëª¨ë“  ìƒì„¸ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©ì ë‹µë³€ì— ì§ì ‘, ì™„ì „í•˜ê²Œ í¬í•¨**ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
    
    4.  **(ğŸŒŸí†µí•© ë° ì í•©ì„± ê²€í† ):**
        * **[ë‹¤ì¤‘ ê²°ê³¼ í†µí•©]:** ë§Œì•½ ì—¬ëŸ¬ ê°œì˜ ê´€ë ¨ ì •ì±…(`{context}`)ì´ ê²€ìƒ‰ë˜ì—ˆë‹¤ë©´, ì´ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ **í•µì‹¬ ì •ì±… 1~2ê°œ**ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”.
        * **[ìœ íš¨ì„± ê²€í† ]:** ì •ì±… ì •ë³´ì— ìœ íš¨ ê¸°ê°„ì´ë‚˜ ëŒ€ìƒ ì—°ë ¹ì´ ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´, ë‹µë³€ ì‹œ í˜„ì¬ ì‹œì ì„ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ì •ë³´ê°€ **ìœ íš¨í•œì§€** ì–¸ê¸‰í•˜ê±°ë‚˜ **'ë°˜ë“œì‹œ ìµœì¢… í™•ì¸ í•„ìš”'**ë¼ëŠ” ì•ˆë‚´ë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©ìì˜ í˜¼ë€ì„ ë°©ì§€í•´ì•¼ í•©ë‹ˆë‹¤.

    5.  **(âœ…ì •í™•ì„± ë° ì•ˆì „ì„± í™•ë³´):**
        * ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ [ì •ì±… ë°ì´í„°]ì— ê·¼ê±°í•´ì•¼ í•˜ë©°, ìë£Œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
        * **[ë§í¬ ê·œì¹™]:** 'ì‹ ì²­ë°©ë²•' ë“±ì— 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ URLì´ ëª…í™•íˆ í¬í•¨ëœ ê²½ìš°ì—ë§Œ í•´ë‹¹ ë§í¬ë¥¼ ì œì‹œí•˜ë©°, **ìë£Œì— ì‹¤ì œ URLì´ ì—†ë‹¤ë©´ ì ˆëŒ€ ê°€ìƒì˜ ë§í¬ë¥¼ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.**

    6.  **ì •ì¤‘í•œ ê±°ì ˆ:** [ì •ì±… ë°ì´í„°]ë¥¼ ê²€í† í•´ë„ ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ì±… ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?"ì™€ ê°™ì´ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

    7.  **(â¡ï¸ë‹¤ìŒ í–‰ë™ ìœ ë„):** ë‹µë³€ì„ ì™„ë£Œí•œ í›„, ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ìœ ìš©í•  ë§Œí•œ **ë‹¤ìŒ ë‹¨ê³„(Next Step)**ë¥¼ ì œì•ˆí•˜ë©° ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤. (ì˜ˆ: "í˜¹ì‹œ ì‹ ì²­ ì ˆì°¨ë‚˜ ë¬¸ì˜ì²˜ ì •ë³´ê°€ í•„ìš”í•˜ì‹ ê°€ìš”?", "ê´€ë ¨ëœ ë‹¤ë¥¸ ì¼ìë¦¬ ì§€ì› ì •ì±…ì— ëŒ€í•´ì„œë„ ì•Œì•„ë³¼ê¹Œìš”?")

    ---
    (ì°¸ê³ : ì•„ë˜ [ì´ì „ ëŒ€í™” ê¸°ë¡]ì€ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ë§¥ë½ ì •ë³´ì…ë‹ˆë‹¤.)
    
    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    (ì°¸ê³ : ì•„ë˜ [ì •ì±… ë°ì´í„°]ëŠ” í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.)

    [ì •ì±… ë°ì´í„°]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ì¹œì ˆí•œ ë‹µë³€]
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "chat_history", "question"]
    )

    # â­ï¸ ìˆ˜ì •ë¨ 2/7: 'RetrievalQA' ëŒ€ì‹  'ConversationalRetrievalChain' ì‚¬ìš©
    # ì´ ì²´ì¸ì€ 'question'ê³¼ 'chat_history'ë¥¼ ì…ë ¥ë°›ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        # 'combine_docs_chain_kwargs'ë¥¼ í†µí•´ RAG í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
        combine_docs_chain_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ.")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤, Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

# Node.jsë¡œë¶€í„° ë°›ì„ ë°ì´í„° ëª¨ë¸ (ì´ì „ê³¼ ë™ì¼)
class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []

# Node.jsì—ê²Œ ë³´ë‚¼ ë°ì´í„° ëª¨ë¸ (ì´ì „ê³¼ ë™ì¼)
class ChatResponse(BaseModel):
    answer: str
    source: str | None = None

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    """
    Node.js ë°±ì—”ë“œë¡œë¶€í„° ì§ˆë¬¸ê³¼ ëŒ€í™” ê¸°ë¡ì„ ë°›ì•„ RAG ì±—ë´‡ì„ ì‹¤í–‰í•˜ê³  ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        user_message = request.message
        chat_history_list = request.history
        
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ëŒ€í™” ê¸°ë¡ ìˆ˜: {len(chat_history_list)}ê°œ")

        # â­ï¸ ìˆ˜ì •ë¨ 3/7: 'history' í˜•ì‹ì„ List[Dict]ì—ì„œ List[Tuple[str, str]]ë¡œ ë³€í™˜
        # (ConversationalRetrievalChainì´ ìš”êµ¬í•˜ëŠ” í˜•ì‹)
        # ë³€í™˜ ëŒ€ìƒ: [{'sender': 'user', 'text': 'Q1'}, {'sender': 'bot', 'text': 'A1'}]
        # ë³€í™˜ ê²°ê³¼: [('Q1', 'A1')]
        formatted_history = []
        user_msg = None
        for turn in chat_history_list:
            if turn.get("sender") == "user":
                user_msg = turn.get("text", "")
            elif turn.get("sender") == "bot" and user_msg is not None:
                # 'user' ë©”ì‹œì§€ ë‹¤ìŒì— 'bot' ë©”ì‹œì§€ê°€ ì˜¤ë©´ ì§ì„ ì´ë¤„ ì¶”ê°€
                formatted_history.append((user_msg, turn.get("text", "")))
                user_msg = None # ë‹¤ìŒ ì§ì„ ìœ„í•´ ì´ˆê¸°í™”

        # â­ï¸ ìˆ˜ì •ë¨ 4/7: 'invoke'ì— 'question'ê³¼ 'chat_history' (íŠœí”Œ ë¦¬ìŠ¤íŠ¸) ì „ë‹¬
        response = qa_chain.invoke({
            "question": user_message, 
            "chat_history": formatted_history
        })
        
        # â­ï¸ ìˆ˜ì •ë¨ 5/7: 'ConversationalRetrievalChain'ì˜ ë‹µë³€ í‚¤ëŠ” 'result'ê°€ ì•„ë‹Œ 'answer'
        bot_reply = response['answer']
        
        source_doc = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        if response.get('source_documents'):
            source_doc = response['source_documents'][0].metadata.get('policy_name', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')

        print(f"LLMì´ ìƒì„±í•œ ë‹µë³€: {bot_reply}")
        print(f"ë‹µë³€ ê·¼ê±°: {source_doc}")

        # Node.jsì—ê²Œ JSON í˜•íƒœë¡œ ë‹µë³€ ë°˜í™˜
        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, Python RAG ì„œë²„ì—ì„œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# --- 5. API ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    
    # --- â¬‡ï¸ (ìˆ˜ì •ë¨) ì„œë²„ ì‹œì‘ ì „, RAG ì²´ì¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ (ë©€í‹°í„´ ë°˜ì˜) â¬‡ï¸ ---
    print("--- [RAG ì²´ì¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì‹œì‘] ---")
    try:
        test_query = "ê·¸ëŸ¼ ìê²© ì¡°ê±´ì€ ë­ì•¼?" 
        
        # â­ï¸ ìˆ˜ì •ë¨ 6/7: í…ŒìŠ¤íŠ¸ìš© 'chat_history'ë¥¼ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
        test_history_tuples = [
            ("ê´‘ì£¼ê´‘ì—­ì‹œ ì²­ë…„ ì •ì±… ì•Œë ¤ì¤˜", "ë„¤, ê´‘ì£¼ê´‘ì—­ì‹œ ì²­ë…„ ì •ì±…ìœ¼ë¡œëŠ” ... (ê°€ìƒ ë‹µë³€) ... ì´ ìˆìŠµë‹ˆë‹¤.")
        ]
        
        test_response = qa_chain.invoke({
            "question": test_query,
            "chat_history": test_history_tuples
        })
        
        print(f"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_query}")
        # â­ï¸ ìˆ˜ì •ë¨ 7/7: í…ŒìŠ¤íŠ¸ ë‹µë³€ í‚¤ë„ 'answer'ë¡œ ë³€ê²½
        print(f"í…ŒìŠ¤íŠ¸ ë‹µë³€: {test_response['answer']}")
        
        if test_response.get('source_documents'):
            print(f"í…ŒìŠ¤íŠ¸ ê·¼ê±°: {test_response['source_documents'][0].metadata.get('policy_name', 'N/A')}")
        else:
            print("í…ŒìŠ¤íŠ¸ ê·¼ê±°: (ê·¼ê±° ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•¨)")
            
        print("--- [âœ… RAG ì²´ì¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì„±ê³µ] ---")

    except Exception as e:
        print(f"--- [ğŸš¨ RAG ì²´ì¸ ì§ì ‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨] ---")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("-----------------------------------")
    # --- â¬†ï¸ í…ŒìŠ¤íŠ¸ ì½”ë“œ ì¢…ë£Œ â¬†ï¸ ---

    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)