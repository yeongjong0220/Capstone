import os
from fastapi import FastAPI
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain 
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever # â­ï¸ í•µì‹¬ ëª¨ë“ˆ ì¶”ê°€
from typing import List, Dict, Any

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. âœŒï¸ ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì„¤ì • ---
INDEX_NAME_POLICY = "policy-chatbot"      # ê¸°ì¡´ ì •ì±… ë°ì´í„°
INDEX_NAME_JOB = "job-postings-index"     # ì‹ ê·œ ì±„ìš© ê³µê³  ë°ì´í„°
# ---------------------------------------------


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    llm = ChatOpenAI(
        model_name="gpt-3.5-turbo",
        temperature=0.0
    )

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ---------------------------------------------------------
    # ğŸ” 1ë²ˆ ê²€ìƒ‰ê¸°: ì •ì±… ë°ì´í„° (policy-chatbot)
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 1 ì—°ê²° ì¤‘: {INDEX_NAME_POLICY}")
    vectorstore_policy = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_POLICY,
        embedding=embeddings,
        text_key="embedding_text" # âš ï¸ ê¸°ì¡´ ì •ì±… ë°ì´í„°ì˜ í‚¤ (ë³€ê²½ ê¸ˆì§€)
    )
    retriever_policy = vectorstore_policy.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 2} # ì •ì±…ì—ì„œ 2ê°œ ê²€ìƒ‰
    )

    # ---------------------------------------------------------
    # ğŸ” 2ë²ˆ ê²€ìƒ‰ê¸°: ì±„ìš© ê³µê³  (job-postings-index)
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 2 ì—°ê²° ì¤‘: {INDEX_NAME_JOB}")
    vectorstore_job = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_JOB,
        embedding=embeddings,
        text_key="context_text" # âš ï¸ ì‹ ê·œ ì±„ìš© ë°ì´í„°ì˜ í‚¤ (ë³€ê²½ ê¸ˆì§€)
    )
    retriever_job = vectorstore_job.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 2} # ì±„ìš© ê³µê³ ì—ì„œ 2ê°œ ê²€ìƒ‰
    )

    # ---------------------------------------------------------
    # ğŸ¤ ì•™ìƒë¸” ê²€ìƒ‰ê¸° (ë‘ ê²°ê³¼ë¥¼ í•©ì¹¨)
    # ---------------------------------------------------------
    print("ğŸ”— ë‘ ê²€ìƒ‰ê¸°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©(Ensemble)í•©ë‹ˆë‹¤...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[retriever_policy, retriever_job],
        weights=[0.5, 0.5] # ì¤‘ìš”ë„ë¥¼ 5:5ë¡œ ì„¤ì •
    )


    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (7ì›ì¹™ ìœ ì§€)
    prompt_template = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ 'ì§€ì—­ ì •ì±…' ë° 'ì±„ìš© ê³µê³ 'ë¥¼ ì‰½ê³  ì¹œì ˆí•˜ê²Œ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.
    í•­ìƒ ì‚¬ìš©ìì˜ ê´€ì ì—ì„œ ìƒê°í•˜ë©°, ëª…í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë§íˆ¬ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    [ë‹µë³€ ìƒì„± 7ì›ì¹™]
    1.  **ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ë§íˆ¬:** í•­ìƒ ìƒëƒ¥í•˜ê³  ì¹œì ˆí•œ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ë©°, ë‚´ìš©ì„ ì „ë‹¬í•  ë•ŒëŠ” **ê°ê´€ì ì´ê³  ê¶Œìœ„ ìˆëŠ”** ìš©ì–´ë¥¼ ì‚¬ìš©í•´ ì‹ ë¢°ê°ì„ ì£¼ì„¸ìš”.
    2.  **ê°€ë…ì„± ìˆëŠ” í˜•ì‹:** ë‹µë³€ì´ ê¸¸ì–´ì§ˆ ê²½ìš°, **ì¤„ë°”ê¿ˆ**, **ê¸€ë¨¸ë¦¬ ê¸°í˜¸(â€¢)**, **ë²ˆí˜¸ ë§¤ê¸°ê¸°**ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•´ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤.
    
    3.  **(ğŸš¨ì •ë³´ ì§ì ‘ ì œì‹œ ë° ì™„ì „ì„±):**
        * **ì ˆëŒ€ë¡œ ë‹µë³€ ë‚´ìš©ì— ë‚´ë¶€ ìš©ì–´ì¸ '[ì •ì±… ë°ì´í„°]', '[ì°¸ê³  ìë£Œ]', ë˜ëŠ” 'ìì„¸í•œ ë‚´ìš©ì€ ìë£Œë¥¼ í™•ì¸í•˜ì„¸ìš”'ì™€ ê°™ì€ íšŒí”¼ì„± ë¬¸êµ¬ë¥¼ ì–¸ê¸‰í•˜ê±°ë‚˜ í¬í•¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.**
        * **[ì •ì±… ë°ì´í„°]ì— í¬í•¨ëœ 'ì œëª©(ì •ì±…ëª…)', 'ëŒ€ìƒ', 'ë‚´ìš©', 'ì‹ ì²­ë°©ë²•', 'ë¬¸ì˜ì²˜', 'ë§í¬' ë“±**ì˜ **ëª¨ë“  ìƒì„¸ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì‚¬ìš©ì ë‹µë³€ì— ì§ì ‘, ì™„ì „í•˜ê²Œ í¬í•¨**ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
    
    4.  **(ğŸŒŸí†µí•© ë° ì í•©ì„± ê²€í† ):**
        * **[ë‹¤ì¤‘ ê²°ê³¼ í†µí•©]:** ë§Œì•½ ì—¬ëŸ¬ ê°œì˜ ê´€ë ¨ ì •ë³´(`{context}`)ê°€ ê²€ìƒ‰ë˜ì—ˆë‹¤ë©´, ì´ë¥¼ í†µí•©í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ **í•µì‹¬ ì •ë³´ 1~2ê°œ**ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì„¸ìš”.
        * **[ìœ íš¨ì„± ê²€í† ]:** ì •ë³´ì— ìœ íš¨ ê¸°ê°„ì´ë‚˜ ë§ˆê°ì¼ì´ ëª…ì‹œë˜ì–´ ìˆë‹¤ë©´, í˜„ì¬ ìœ íš¨í•œì§€ ì–¸ê¸‰í•˜ê±°ë‚˜ í™•ì¸ì´ í•„ìš”í•¨ì„ ì•ˆë‚´í•˜ì„¸ìš”.

    5.  **(âœ…ì •í™•ì„± ë° ì•ˆì „ì„± í™•ë³´):**
        * ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ [ì •ì±… ë°ì´í„°]ì— ê·¼ê±°í•´ì•¼ í•˜ë©°, ìë£Œì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
        * **[ë§í¬ ê·œì¹™]:** 'ì‹ ì²­ë°©ë²•'ì´ë‚˜ 'ë§í¬' ë“±ì— 'http://' ë˜ëŠ” 'https://'ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ URLì´ ëª…í™•íˆ í¬í•¨ëœ ê²½ìš°ì—ë§Œ í•´ë‹¹ ë§í¬ë¥¼ ì œì‹œí•˜ë©°, **ìë£Œì— ì‹¤ì œ URLì´ ì—†ë‹¤ë©´ ì ˆëŒ€ ê°€ìƒì˜ ë§í¬ë¥¼ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.**

    6.  **ì •ì¤‘í•œ ê±°ì ˆ:** [ì •ì±… ë°ì´í„°]ë¥¼ ê²€í† í•´ë„ ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?"ì™€ ê°™ì´ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

    7.  **(â¡ï¸ë‹¤ìŒ í–‰ë™ ìœ ë„):** ë‹µë³€ì„ ì™„ë£Œí•œ í›„, ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ìœ ìš©í•  ë§Œí•œ **ë‹¤ìŒ ë‹¨ê³„(Next Step)**ë¥¼ ì œì•ˆí•˜ë©° ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

    ---
    (ì°¸ê³ : ì•„ë˜ [ì´ì „ ëŒ€í™” ê¸°ë¡]ì€ ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ë§¥ë½ ì •ë³´ì…ë‹ˆë‹¤.)
    
    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    (ì°¸ê³ : ì•„ë˜ [ì •ì±… ë°ì´í„°]ëŠ” í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.)

    [ì •ì±… ë°ì´í„°]
    {context}

    [ì§ˆë¬¸]
    {question}

    [ì¹œì ˆí•œ ë‹µë³€]
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "chat_history", "question"]
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        # â­ï¸ ì—¬ê¸°ì„œ ì•™ìƒë¸” ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        retriever=ensemble_retriever,
        combine_docs_chain_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (ì •ì±…+ì±„ìš© í†µí•©).")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤, Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []

class ChatResponse(BaseModel):
    answer: str
    source: str | None = None

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    try:
        user_message = request.message
        chat_history_list = request.history
        
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")

        formatted_history = []
        user_msg = None
        for turn in chat_history_list:
            if turn.get("sender") == "user":
                user_msg = turn.get("text", "")
            elif turn.get("sender") == "bot" and user_msg is not None:
                formatted_history.append((user_msg, turn.get("text", "")))
                user_msg = None 

        response = qa_chain.invoke({
            "question": user_message, 
            "chat_history": formatted_history
        })
        
        bot_reply = response['answer']
        
        # ì¶œì²˜ í‘œì‹œ ë¡œì§ ê°œì„  (ì—¬ëŸ¬ ì¸ë±ìŠ¤ì—ì„œ ì„ì—¬ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
        source_doc = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        if response.get('source_documents'):
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata = response['source_documents'][0].metadata
            # 1. ì±„ìš© ê³µê³ ì¸ ê²½ìš° 'title'
            # 2. ì •ì±… ë°ì´í„°ì¸ ê²½ìš° 'policy_name'
            source_doc = metadata.get('title') or metadata.get('policy_name', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')

        print(f"LLMì´ ìƒì„±í•œ ë‹µë³€: {bot_reply}")
        print(f"ë‹µë³€ ê·¼ê±°: {source_doc}")

        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, Python RAG ì„œë²„ì—ì„œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# --- 5. API ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    
    print("--- [RAG ì²´ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘] ---")
    try:
        # í…ŒìŠ¤íŠ¸ 1: ì •ì±… ì§ˆë¬¸
        test_query_1 = "ê´‘ì£¼ ì²­ë…„ ì •ì±… ì•Œë ¤ì¤˜"
        print(f"\n[í…ŒìŠ¤íŠ¸ 1] ì§ˆë¬¸: {test_query_1}")
        resp1 = qa_chain.invoke({"question": test_query_1, "chat_history": []})
        print(f"ë‹µë³€: {resp1['answer'][:50]}...") # ë„ˆë¬´ ê¸°ë‹ˆê¹Œ ì•ë¶€ë¶„ë§Œ

        # í…ŒìŠ¤íŠ¸ 2: ì±„ìš© ì§ˆë¬¸ (ì‹œì—°ìš© ê³µê³ ê°€ ìˆë‹¤ê³  ê°€ì •)
        test_query_2 = "ì‹ ì… ê°œë°œì ì±„ìš© ê³µê³  ìˆì–´?"
        print(f"\n[í…ŒìŠ¤íŠ¸ 2] ì§ˆë¬¸: {test_query_2}")
        resp2 = qa_chain.invoke({"question": test_query_2, "chat_history": []})
        print(f"ë‹µë³€: {resp2['answer'][:50]}...")

        print("\n--- [âœ… RAG ì²´ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ] ---")

    except Exception as e:
        print(f"--- [ğŸš¨ RAG ì²´ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨] ---")
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("-----------------------------------")

    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)