import os
from fastapi import FastAPI
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv

# 1. ì„ë² ë”©ì€ OpenAI ìœ ì§€
from langchain_openai import OpenAIEmbeddings 

# 2. LLMì€ Google Gemini ì‚¬ìš©
from langchain_google_genai import ChatGoogleGenerativeAI 

from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain 
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever 
from typing import List, Dict, Any

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("GOOGLE_API_KEY"):
    raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. âœŒï¸ ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì„¤ì • ---
INDEX_NAME_POLICY = "policy-chatbot"      # ê¸°ì¡´ ì •ì±… ë°ì´í„°
INDEX_NAME_JOB = "job-postings-index"     # ì‹ ê·œ ì±„ìš© ê³µê³ 
# ---------------------------------------------


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    # ğŸŒŸ [ëª¨ë¸] Gemini 2.0 Flash Exp
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp", 
        temperature=0.0
    )

    # âš ï¸ [ì„ë² ë”©] OpenAI
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ---------------------------------------------------------
    # ğŸ” 1ë²ˆ ê²€ìƒ‰ê¸°: ì •ì±… ë°ì´í„°
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 1 ì—°ê²° ì¤‘: {INDEX_NAME_POLICY}")
    vectorstore_policy = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_POLICY,
        embedding=embeddings,
        text_key="embedding_text"
    )
    retriever_policy = vectorstore_policy.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 4}  # ğŸŒŸ ê²€ìƒ‰ ê°œìˆ˜ë¥¼ ì¡°ê¸ˆ ëŠ˜ë ¤ ì •ë³´ë¥¼ ë” ë§ì´ í™•ë³´
    )

    # ---------------------------------------------------------
    # ğŸ” 2ë²ˆ ê²€ìƒ‰ê¸°: ì±„ìš© ê³µê³ 
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 2 ì—°ê²° ì¤‘: {INDEX_NAME_JOB}")
    vectorstore_job = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_JOB,
        embedding=embeddings,
        text_key="context_text"
    )
    retriever_job = vectorstore_job.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} 
    )

    # ---------------------------------------------------------
    # ğŸ¤ ì•™ìƒë¸” ê²€ìƒ‰ê¸° (í†µí•©)
    # ---------------------------------------------------------
    print("ğŸ”— ë‘ ê²€ìƒ‰ê¸°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©(Ensemble)í•©ë‹ˆë‹¤...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[retriever_policy, retriever_job],
        weights=[0.6, 0.4] # ğŸŒŸ ì •ì±… ê²€ìƒ‰ ë¹„ì¤‘ì„ ì•½ê°„ ë†’ì„
    )


    # ğŸŒŸ [í”„ë¡¬í”„íŠ¸ ìˆ˜ì •] ë§¤ì¹­ ë…¼ë¦¬ ëŒ€í­ ê°•í™”
    prompt_template = """
    ë‹¹ì‹ ì˜ ì´ë¦„ì€ **'Jobs'**ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ 'ì§€ì—­ ì •ì±…' ë° 'ì±„ìš© ê³µê³ 'ë¥¼ ëª…í™•í•˜ê³  ì‹ ë¢°ê° ìˆê²Œ ì•ˆë‚´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ í™”ë©´ì—ëŠ” í…ìŠ¤íŠ¸ë§Œ í‘œì‹œë˜ë¯€ë¡œ, **íŠ¹ìˆ˜ë¬¸ìë‚˜ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ ** ì¤„ê¸€ í˜•íƒœë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    [ğŸ”´ íŠ¹ë³„ ì§€ì¹¨: ì¸ì‚¬ ë° ìê¸°ì†Œê°œ]
    * ì‚¬ìš©ìê°€ "ì•ˆë…•", "ë°˜ê°€ì›Œ", "ëˆ„êµ¬ë‹ˆ" ë“± ë‹¨ìˆœ ì¸ì‚¬ë¥¼ í•˜ê±°ë‚˜ ìê¸°ì†Œê°œë¥¼ ìš”ì²­í•  ê²½ìš°:
        1. **ì ˆëŒ€** [ì •ì±… ë°ì´í„°]ë‚˜ [ê²€ìƒ‰ ê²°ê³¼]ë¥¼ ì–µì§€ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
        2. **"ì•ˆë…•í•˜ì„¸ìš”, Jobsì…ë‹ˆë‹¤. ì°¾ìœ¼ì‹œëŠ” ì •ì±…ì´ë‚˜ í˜œíƒì´ ìˆë‹¤ë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë´ ì£¼ì„¸ìš”."** ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
    {user_context_prompt}

    [ğŸ”µ í•µì‹¬ ì§€ì¹¨: ë§ì¶¤í˜• ì •ì±… ì¶”ì²œ (ê°€ì¥ ì¤‘ìš”)]
    ì‚¬ìš©ìê°€ "ë‚´ ë‚˜ì´ì— ë§ëŠ”", "ë‚´ ì§€ì—­ ì •ì±…" ê°™ì´ ë³¸ì¸ì˜ ì¡°ê±´ì— ë§ëŠ” ì •ë³´ë¥¼ ìš”ì²­í–ˆì„ ë•Œ:
    1. ìœ„ [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]ì˜ 'ë‚˜ì´'ì™€ 'ê±°ì£¼ì§€'ë¥¼ í™•ì¸í•˜ì„¸ìš”.
    2. ì•„ë˜ [ì •ì±… ë°ì´í„°]ì˜ ë‚´ìš© ì¤‘ 'ì§€ì›ëŒ€ìƒ', 'ìê²©ìš”ê±´', 'ì§€ì—­' ë“±ì˜ ë‚´ìš©ì„ ê¼¼ê¼¼íˆ ì½ìœ¼ì„¸ìš”.
    3. **ì‚¬ìš©ìì˜ ë‚˜ì´ì™€ ì§€ì—­ì´ ì •ì±…ì˜ ìê²© ìš”ê±´ì— í¬í•¨ë˜ëŠ”ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.**
       (ì˜ˆ: ì‚¬ìš©ì '24ì„¸' -> ì •ì±… '19ì„¸~39ì„¸'ë¼ë©´ **ë§¤ì¹­ ì„±ê³µ**ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¶”ì²œ)
    4. ë§Œì•½ ê²€ìƒ‰ëœ ë°ì´í„°ê°€ ì‚¬ìš©ìì˜ ì§€ì—­(ì˜ˆ: ê´‘ì£¼)ê³¼ ë‹¤ë¥´ë‹¤ë©´, "í˜„ì¬ ê±°ì£¼í•˜ì‹œëŠ” ì§€ì—­ì— ë”± ë§ëŠ” ì •ë³´ëŠ” ì—†ì§€ë§Œ, ìœ ì‚¬í•œ ë‹¤ë¥¸ ì§€ì—­ ì •ë³´ëŠ” ìˆìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë§í•˜ì„¸ìš”.

    [ğŸš« ë°ì´í„° í•„í„°ë§ ë° ê´€ë ¨ì„± ê²€ì¦ ê·œì¹™]
    1. **ì•ŒíŒŒë²³ ì½”ë“œ ì‚­ì œ:** 'ë¶„ì•¼', 'ì‹ ì²­ë°©ë²•' ë“±ì´ 'A', 'B' ë“± ì˜ë¯¸ ì—†ëŠ” ì•ŒíŒŒë²³ì¸ ê²½ìš° ìƒëµí•˜ì„¸ìš”.
    2. **ë™ë¬¸ì„œë‹µ ê¸ˆì§€:** ì •ì±… ì§ˆë¬¸ì— ëœ¬ê¸ˆì—†ëŠ” ì±„ìš© ê³µê³ ë¥¼ ì„ì–´ì„œ ë‹µí•˜ì§€ ë§ˆì„¸ìš”. ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” ì •ë³´ë§Œ ê³¨ë¼ë‚´ì„¸ìš”.

    ---
    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    [ì •ì±… ë° ì±„ìš© ë°ì´í„° (ê²€ìƒ‰ ê²°ê³¼)]
    {context}

    [ì§ˆë¬¸]
    {question}

    [Jobsì˜ ë‹µë³€ (ë§ˆí¬ë‹¤ìš´ ì—†ì´, ë§¤ì¹­ëœ ì •ì±… ìœ„ì£¼ë¡œ)]
    """
    
    PROMPT = PromptTemplate(
        template=prompt_template, 
        input_variables=["context", "chat_history", "question", "user_context_prompt"]
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, 
        retriever=ensemble_retriever,
        combine_docs_chain_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (Gemini 2.0 Exp + OpenAI Embedding).")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤(.env), Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []
    user_profile: Dict[str, Any] = {}

class ChatResponse(BaseModel):
    answer: str
    source: str | None = None

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    try:
        user_message = request.message
        chat_history_list = request.history
        user_profile = request.user_profile 
        
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")
        print(f"ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}") 

        # ì‚¬ìš©ì ì •ë³´ í¬ë§¤íŒ…
        user_context_str = ""
        user_keywords = "" # ğŸŒŸ ê²€ìƒ‰ì–´ ë³´ì •ì„ ìœ„í•œ í‚¤ì›Œë“œ ë¬¸ìì—´
        
        if user_profile and (user_profile.get("age") or user_profile.get("region")):
            info_list = []
            if user_profile.get("age") and user_profile.get("age") != "ì•Œ ìˆ˜ ì—†ìŒ":
                age_val = user_profile['age']
                info_list.append(f"- ë‚˜ì´: {age_val}")
                user_keywords += f" {age_val}" # ê²€ìƒ‰ì–´ì— ì¶”ê°€
            if user_profile.get("region") and user_profile.get("region") != "ì•Œ ìˆ˜ ì—†ìŒ":
                region_val = user_profile['region']
                info_list.append(f"- ê±°ì£¼ì§€: {region_val}")
                user_keywords += f" {region_val}" # ê²€ìƒ‰ì–´ì— ì¶”ê°€
            
            if info_list:
                user_context_str = "\n".join(info_list)
            else:
                user_context_str = "(ì‚¬ìš©ì ì •ë³´ ì—†ìŒ)"
        else:
            user_context_str = "(ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ë˜ëŠ” ì •ë³´ ì—†ìŒ)"

        formatted_history = []
        user_msg = None
        for turn in chat_history_list:
            if turn.get("sender") == "user":
                user_msg = turn.get("text", "")
            elif turn.get("sender") == "bot" and user_msg is not None:
                formatted_history.append((user_msg, turn.get("text", "")))
                user_msg = None 

        # ğŸŒŸ [í•µì‹¬ ìˆ˜ì •] ê²€ìƒ‰ì–´ ë³´ì • (Query Augmentation)
        # ì‚¬ìš©ìê°€ "ë‚´ ì •ë³´" ê°™ì€ ëŒ€ëª…ì‚¬ë¥¼ ì¼ì„ ë•Œ, ì‹¤ì œ ì§€ì—­/ë‚˜ì´ í‚¤ì›Œë“œë¥¼ ë¶™ì—¬ì„œ ê²€ìƒ‰ê¸°ì— ì „ë‹¬
        search_query = user_message
        if "ë‚˜" in user_message or "ë‚´" in user_message or "ì¡°ê±´" in user_message or "ì¶”ì²œ" in user_message:
            search_query += f" {user_keywords}"
            print(f"ğŸ” ë³´ì •ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

        # LLM í˜¸ì¶œ
        response = qa_chain.invoke({
            "question": search_query, # ğŸŒŸ ë³´ì •ëœ ì¿¼ë¦¬ ì‚¬ìš©
            "chat_history": formatted_history,
            "user_context_prompt": user_context_str 
        })
        
        bot_reply = response['answer']
        
        # ì¶œì²˜ í‘œì‹œ ë¡œì§
        source_doc = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        if response.get('source_documents'):
            metadata = response['source_documents'][0].metadata
            source_doc = metadata.get('title') or metadata.get('policy_name', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')

        print(f"Gemini ë‹µë³€: {bot_reply}")
        print(f"ë‹µë³€ ê·¼ê±°: {source_doc}")

        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# --- 5. API ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)