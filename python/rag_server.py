import os
from fastapi import FastAPI
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv

# 1. ì„ë² ë”©ì€ OpenAI ìœ ì§€
from langchain_openai import OpenAIEmbeddings 

# 2. LLMì€ Google Gemini ì‚¬ìš©
from langchain_google_genai import ChatGoogleGenerativeAI 

from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain 
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever 
from typing import List, Dict, Any

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("GOOGLE_API_KEY"):
    raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. âœŒï¸ ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì„¤ì • ---
INDEX_NAME_POLICY = "policy-chatbot"      # ê¸°ì¡´ ì •ì±… ë°ì´í„°
INDEX_NAME_JOB = "job-postings-index"     # ì‹ ê·œ ì±„ìš© ê³µê³ 
# ---------------------------------------------


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    # ğŸŒŸ [ëª¨ë¸] Gemini 2.0 Flash Exp
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp", 
        temperature=0.0
    )

    # âš ï¸ [ì„ë² ë”©] OpenAI
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ---------------------------------------------------------
    # ğŸ” 1ë²ˆ ê²€ìƒ‰ê¸°: ì •ì±… ë°ì´í„°
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 1 ì—°ê²° ì¤‘: {INDEX_NAME_POLICY}")
    vectorstore_policy = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_POLICY,
        embedding=embeddings,
        text_key="embedding_text"
    )
    retriever_policy = vectorstore_policy.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 4} 
    )

    # ---------------------------------------------------------
    # ğŸ” 2ë²ˆ ê²€ìƒ‰ê¸°: ì±„ìš© ê³µê³ 
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 2 ì—°ê²° ì¤‘: {INDEX_NAME_JOB}")
    vectorstore_job = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_JOB,
        embedding=embeddings,
        text_key="context_text"
    )
    retriever_job = vectorstore_job.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} 
    )

    # ---------------------------------------------------------
    # ğŸ¤ ì•™ìƒë¸” ê²€ìƒ‰ê¸° (í†µí•©)
    # ---------------------------------------------------------
    print("ğŸ”— ë‘ ê²€ìƒ‰ê¸°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©(Ensemble)í•©ë‹ˆë‹¤...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[retriever_policy, retriever_job],
        weights=[0.6, 0.4] 
    )


    # ğŸŒŸ [í”„ë¡¬í”„íŠ¸ ìˆ˜ì •] 'ì•ŒíŒŒë²³ ì œê±°' ê·œì¹™ì„ ìµœìƒë‹¨ ê°•ë ¥ ì§€ì¹¨ìœ¼ë¡œ ì´ë™
    prompt_template = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ 'ì§€ì—­ ì •ì±…' ë° 'ì±„ìš© ê³µê³ 'ë¥¼ ì•ˆë‚´í•˜ëŠ” ë˜‘ë˜‘í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸, **'Jobs(ì¡ìŠ¤)'**ì…ë‹ˆë‹¤.
    
    [ğŸ§¹ ë°ì´í„° ì •ì œ ë° í•„í„°ë§ ê·œì¹™ (ìµœìš°ì„  ì ìš©)]
    1. **ë¬´ì˜ë¯¸í•œ ì•ŒíŒŒë²³/ê¸°í˜¸ ì ˆëŒ€ ë°œì„¤ ê¸ˆì§€:** - ë°ì´í„°ì— "ì‹ ì²­ë°©ë²•: A", "ë¶„ì•¼: B", "ë¹„ê³ : -" ì²˜ëŸ¼ ì˜ë¯¸ ì—†ëŠ” ì•ŒíŒŒë²³ì´ë‚˜ ê¸°í˜¸ë§Œ ì í˜€ ìˆë‹¤ë©´, **í•´ë‹¹ í•­ëª©ì€ ì•„ì˜ˆ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.**
       - âŒ ë‚˜ìœ ì˜ˆ: "ì‹ ì²­ ë°©ë²•ì€ Aì…ë‹ˆë‹¤."
       - â­• ì¢‹ì€ ì˜ˆ: (ì‹ ì²­ ë°©ë²• ìì²´ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•ŠìŒ)
    2. ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì–µì§€ë¡œ ë¼ì›Œ ë§ì¶”ì§€ ë§ê³ , í™•ì‹¤í•œ ë‚´ìš©ë§Œ ì „ë‹¬í•˜ì„¸ìš”.

    [âœ¨ ìƒí™©ë³„ ë‹µë³€ ê°€ì´ë“œ]
    **Case 1. ì‚¬ìš©ìê°€ "ì•ˆë…•", "ëˆ„êµ¬ì•¼" ë“± ì¸ì‚¬ë¥¼ í•  ë•Œ:**
    - "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Jobsì…ë‹ˆë‹¤. ì‚¬ìš©ìë‹˜ì˜ ë‚˜ì´ì™€ ì‚¬ëŠ” ê³³ì„ ë¶„ì„í•´ì„œ ë”± ë§ëŠ” ì •ì±…ê³¼ ì¼ìë¦¬ë¥¼ ì°¾ì•„ë“œë¦¬ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤." ë¼ê³  ìì‹ ì„ ì†Œê°œí•˜ì„¸ìš”.
    - ì´ë•ŒëŠ” ê²€ìƒ‰ëœ ì •ì±… ì •ë³´ë¥¼ ì–µì§€ë¡œ ë§í•˜ì§€ ë§ˆì„¸ìš”.
    
    **Case 2. ì‚¬ìš©ìê°€ ì •ì±…ì´ë‚˜ ì¼ìë¦¬ ì •ë³´ë¥¼ ë¬¼ì–´ë³¼ ë•Œ:**
    - **ìê¸°ì†Œê°œë¥¼ ìƒëµ**í•˜ê³  ë°”ë¡œ ë³¸ë¡ (ì •ë³´)ìœ¼ë¡œ ë“¤ì–´ê°€ì„¸ìš”.
    - ë‹µë³€ ì˜ˆì‹œ: "ë„¤, (ì§€ì—­)ì— ê±°ì£¼í•˜ì‹œëŠ” (ë‚˜ì´) ì‚¬ìš©ìë‹˜ì„ ìœ„í•œ ì •ë³´ë¥¼ ì°¾ì•„ë³´ì•˜ìŠµë‹ˆë‹¤."

    [ğŸ” ìƒì„¸ ì •ë³´ ë‹µë³€ ê·œì¹™]
    ì‚¬ìš©ìê°€ "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê±°ì•¼?"ë¼ê³  ë˜ë¬¼ì—ˆì„ ë•Œ:
    1. ë°ì´í„°ì— êµ¬ì²´ì  ì˜ˆì‹œê°€ ìˆìœ¼ë©´ ë‚˜ì—´í•˜ì„¸ìš”.
    2. ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë¬¸ì„œì—ëŠ” ìƒì„¸ ë‚´ìš©ì´ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µí•˜ì„¸ìš”.

    [ğŸ”µ ë§ì¶¤í˜• ë§¤ì¹­ ì§€ì¹¨]
    1. [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]ì™€ [ì •ì±… ë°ì´í„°]ì˜ ìê²©ìš”ê±´ì„ ë¹„êµí•˜ì—¬ ë§¤ì¹­ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
    2. ì¡°ê±´ì´ ë§ëŠ” ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí•˜ì„¸ìš”.

    [ğŸš« í˜•ì‹ ì œí•œ]
    * ë§ˆí¬ë‹¤ìš´(Markdown), íŠ¹ìˆ˜ë¬¸ì, **ë³¼ë“œì²´** ì‚¬ìš© ê¸ˆì§€. ì˜¤ì§ ì¤„ê¸€(Text)ë¡œë§Œ ë‹µí•˜ì„¸ìš”.

    ---
    [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
    {user_context_prompt}

    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    [ì •ì±… ë° ì±„ìš© ë°ì´í„° (ê²€ìƒ‰ ê²°ê³¼)]
    {context}

    [ì§ˆë¬¸]
    {question}

    [Jobsì˜ ë‹µë³€ (ìœ„ ë°ì´í„° ì •ì œ ê·œì¹™ ì—„ìˆ˜)]
    """
    
    PROMPT = PromptTemplate(
        template=prompt_template, 
        input_variables=["context", "chat_history", "question", "user_context_prompt"]
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, 
        retriever=ensemble_retriever,
        combine_docs_chain_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (Gemini 2.0 Exp + OpenAI Embedding).")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤(.env), Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []
    user_profile: Dict[str, Any] = {}

class ChatResponse(BaseModel):
    answer: str
    source: str | None = None

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    try:
        user_message = request.message
        chat_history_list = request.history
        user_profile = request.user_profile 
        
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")
        print(f"ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}") 

        # ì‚¬ìš©ì ì •ë³´ í¬ë§¤íŒ…
        user_context_str = ""
        user_keywords = "" 
        
        if user_profile and (user_profile.get("age") or user_profile.get("region")):
            info_list = []
            if user_profile.get("age") and user_profile.get("age") != "ì•Œ ìˆ˜ ì—†ìŒ":
                age_val = user_profile['age']
                info_list.append(f"- ë‚˜ì´: {age_val}")
                user_keywords += f" {age_val}" 
            if user_profile.get("region") and user_profile.get("region") != "ì•Œ ìˆ˜ ì—†ìŒ":
                region_val = user_profile['region']
                info_list.append(f"- ê±°ì£¼ì§€: {region_val}")
                user_keywords += f" {region_val}" 
            
            if info_list:
                user_context_str = "\n".join(info_list)
            else:
                user_context_str = "(ì‚¬ìš©ì ì •ë³´ ì—†ìŒ)"
        else:
            user_context_str = "(ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ë˜ëŠ” ì •ë³´ ì—†ìŒ)"

        formatted_history = []
        user_msg = None
        for turn in chat_history_list:
            if turn.get("sender") == "user":
                user_msg = turn.get("text", "")
            elif turn.get("sender") == "bot" and user_msg is not None:
                formatted_history.append((user_msg, turn.get("text", "")))
                user_msg = None 

        # ğŸŒŸ ê²€ìƒ‰ì–´ ë³´ì • (Query Augmentation)
        search_query = user_message
        if "ë‚˜" in user_message or "ë‚´" in user_message or "ì¡°ê±´" in user_message or "ì¶”ì²œ" in user_message:
            search_query += f" {user_keywords}"
            print(f"ğŸ” ë³´ì •ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

        # LLM í˜¸ì¶œ
        response = qa_chain.invoke({
            "question": search_query, 
            "chat_history": formatted_history,
            "user_context_prompt": user_context_str 
        })
        
        bot_reply = response['answer']
        
        # ì¶œì²˜ í‘œì‹œ ë¡œì§
        source_doc = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        if response.get('source_documents'):
            metadata = response['source_documents'][0].metadata
            source_doc = metadata.get('title') or metadata.get('policy_name', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')

        print(f"Gemini ë‹µë³€: {bot_reply}")
        print(f"ë‹µë³€ ê·¼ê±°: {source_doc}")

        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# --- 5. API ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)