import os
from fastapi import FastAPI
from pydantic import BaseModel
from uvicorn import run
from dotenv import load_dotenv

# 1. ì„ë² ë”©ì€ OpenAI ìœ ì§€
from langchain_openai import OpenAIEmbeddings 

# 2. LLMì€ Google Gemini ì‚¬ìš©
from langchain_google_genai import ChatGoogleGenerativeAI 

from langchain_pinecone import PineconeVectorStore
from langchain.chains import ConversationalRetrievalChain 
from langchain.prompts import PromptTemplate
from langchain.retrievers import EnsembleRetriever 
from typing import List, Dict, Any

# --- 1. .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ ---
load_dotenv()

if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("PINECONE_API_KEY"):
    raise ValueError("PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not os.getenv("GOOGLE_API_KEY"):
    raise ValueError("GOOGLE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# --- 2. âœŒï¸ ë‘ ê°œì˜ ì¸ë±ìŠ¤ ì„¤ì • ---
INDEX_NAME_POLICY = "policy-chatbot"      # ê¸°ì¡´ ì •ì±… ë°ì´í„°
INDEX_NAME_JOB = "job-postings-index"     # ì‹ ê·œ ì±„ìš© ê³µê³ 
# ---------------------------------------------


# --- 3. RAG ì±—ë´‡ í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ---
try:
    print("RAG ì±—ë´‡ êµ¬ì„± ìš”ì†Œë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

    # ğŸŒŸ [ëª¨ë¸] Gemini 2.0 Flash Exp
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp", 
        temperature=0.0
    )

    # âš ï¸ [ì„ë² ë”©] OpenAI
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ---------------------------------------------------------
    # ğŸ” 1ë²ˆ ê²€ìƒ‰ê¸°: ì •ì±… ë°ì´í„°
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 1 ì—°ê²° ì¤‘: {INDEX_NAME_POLICY}")
    vectorstore_policy = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_POLICY,
        embedding=embeddings,
        text_key="embedding_text"
    )
    retriever_policy = vectorstore_policy.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} 
    )

    # ---------------------------------------------------------
    # ğŸ” 2ë²ˆ ê²€ìƒ‰ê¸°: ì±„ìš© ê³µê³ 
    # ---------------------------------------------------------
    print(f"ğŸ“¡ ì¸ë±ìŠ¤ 2 ì—°ê²° ì¤‘: {INDEX_NAME_JOB}")
    vectorstore_job = PineconeVectorStore.from_existing_index(
        index_name=INDEX_NAME_JOB,
        embedding=embeddings,
        text_key="context_text"
    )
    retriever_job = vectorstore_job.as_retriever(
        search_type="similarity",
        search_kwargs={'k': 3} 
    )

    # ---------------------------------------------------------
    # ğŸ¤ ì•™ìƒë¸” ê²€ìƒ‰ê¸° (í†µí•©)
    # ---------------------------------------------------------
    print("ğŸ”— ë‘ ê²€ìƒ‰ê¸°ë¥¼ í•˜ë‚˜ë¡œ í†µí•©(Ensemble)í•©ë‹ˆë‹¤...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[retriever_policy, retriever_job],
        weights=[0.5, 0.5]
    )


    # ğŸŒŸ [í”„ë¡¬í”„íŠ¸ ìˆ˜ì •] 'Jobs' í˜ë¥´ì†Œë‚˜ ì£¼ì…
    prompt_template = """
    ë‹¹ì‹ ì˜ ì´ë¦„ì€ **'Jobs'**ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ 'ì§€ì—­ ì •ì±…' ë° 'ì±„ìš© ê³µê³ 'ë¥¼ ëª…í™•í•˜ê³  ì‹ ë¢°ê° ìˆê²Œ ì•ˆë‚´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ í™”ë©´ì—ëŠ” í…ìŠ¤íŠ¸ë§Œ í‘œì‹œë˜ë¯€ë¡œ, **íŠ¹ìˆ˜ë¬¸ìë‚˜ ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ê³ ** ì¤„ê¸€ í˜•íƒœë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    [ğŸ”´ íŠ¹ë³„ ì§€ì¹¨: ì¸ì‚¬ ë° ìê¸°ì†Œê°œ]
    * ì‚¬ìš©ìê°€ "ì•ˆë…•", "ë°˜ê°€ì›Œ", "ëˆ„êµ¬ë‹ˆ" ë“± ë‹¨ìˆœ ì¸ì‚¬ë¥¼ í•˜ê±°ë‚˜ ìê¸°ì†Œê°œë¥¼ ìš”ì²­í•  ê²½ìš°:
        1. **ì ˆëŒ€** [ì •ì±… ë°ì´í„°]ë‚˜ [ê²€ìƒ‰ ê²°ê³¼]ë¥¼ ì–µì§€ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
        2. **"ì•ˆë…•í•˜ì„¸ìš”, Jobsì…ë‹ˆë‹¤. ì°¾ìœ¼ì‹œëŠ” ì •ì±…ì´ë‚˜ í˜œíƒì´ ìˆë‹¤ë©´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë´ ì£¼ì„¸ìš”."** ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

    [ğŸš« ë°ì´í„° í•„í„°ë§ ë° ê´€ë ¨ì„± ê²€ì¦ ê·œì¹™]
    1.  **ì•ŒíŒŒë²³ ì½”ë“œ ì‚­ì œ:** 'ë¶„ì•¼', 'ì‹ ì²­ë°©ë²•' ë“±ì´ 'A', 'B', 'C' ë“± ì˜ë¯¸ ì—†ëŠ” ì•ŒíŒŒë²³ìœ¼ë¡œë§Œ ëœ ê²½ìš° ë‹µë³€ì—ì„œ ì•„ì˜ˆ ìƒëµí•˜ì„¸ìš”.
    2.  **ë™ë¬¸ì„œë‹µ ê¸ˆì§€:** ì£¼ì œê°€ ë§ì§€ ì•ŠëŠ” ì •ë³´(ì˜ˆ: ì •ì±… ì§ˆë¬¸ì— ì±„ìš© ê³µê³  ë‹µë³€)ëŠ” ì¶”ì²œí•˜ì§€ ë§ê³  ê³¼ê°íˆ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  í•˜ì„¸ìš”.

    [ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´]
    {user_context_prompt}

    [ë‹µë³€ ìƒì„± ì›ì¹™]
    1.  **ë§ì¶¤í˜• ì¶”ì²œ:** ì‚¬ìš©ì í”„ë¡œí•„(ë‚˜ì´, ì§€ì—­)ê³¼ ì¼ì¹˜í•˜ëŠ” ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
    2.  **ê°€ë…ì„±:** ë§ˆí¬ë‹¤ìš´ íƒœê·¸(bold, header ë“±) ì‚¬ìš© ê¸ˆì§€. ì¤„ë°”ê¿ˆë§Œ ì‚¬ìš©.
    3.  **ì •í™•ì„±:** ê²€ìƒ‰ëœ ë°ì´í„°([ì •ì±… ë°ì´í„°])ì— ê¸°ë°˜í•˜ë˜, ìœ„ [ğŸš« ê´€ë ¨ì„± ê²€ì¦ ê·œì¹™]ì„ í†µê³¼í•œ ì •ë³´ë§Œ ë§í•˜ì„¸ìš”.

    ---
    [ì´ì „ ëŒ€í™” ê¸°ë¡]
    {chat_history}
    
    [ì •ì±… ë°ì´í„° (ê²€ìƒ‰ ê²°ê³¼)]
    {context}

    [ì§ˆë¬¸]
    {question}

    [Jobsì˜ ë‹µë³€ (ë§ˆí¬ë‹¤ìš´ ì—†ì´)]
    """
    
    PROMPT = PromptTemplate(
        template=prompt_template, 
        input_variables=["context", "chat_history", "question", "user_context_prompt"]
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm, # Gemini 2.0 Flash Exp
        retriever=ensemble_retriever,
        combine_docs_chain_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )

    print("âœ… RAG ì±—ë´‡ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ (Gemini 2.0 Exp + OpenAI Embedding).")

except Exception as e:
    print(f"ğŸš¨ RAG ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("API í‚¤(.env), Pinecone ì¸ë±ìŠ¤ ì´ë¦„, ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    exit(1)


# --- 4. FastAPI ì„œë²„ ì„¤ì • ---
app = FastAPI()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []
    user_profile: Dict[str, Any] = {}

class ChatResponse(BaseModel):
    answer: str
    source: str | None = None

@app.post("/ask", response_model=ChatResponse)
async def ask_question(request: ChatRequest):
    try:
        user_message = request.message
        chat_history_list = request.history
        user_profile = request.user_profile 
        
        print(f"Node.jsë¡œë¶€í„° ë°›ì€ ì§ˆë¬¸: {user_message}")
        print(f"ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}") 

        # ì‚¬ìš©ì ì •ë³´ í¬ë§¤íŒ…
        user_context_str = ""
        if user_profile and (user_profile.get("age") or user_profile.get("region")):
            info_list = []
            if user_profile.get("age") and user_profile.get("age") != "ì•Œ ìˆ˜ ì—†ìŒ":
                info_list.append(f"- ë‚˜ì´: {user_profile['age']}")
            if user_profile.get("region") and user_profile.get("region") != "ì•Œ ìˆ˜ ì—†ìŒ":
                info_list.append(f"- ê±°ì£¼ì§€: {user_profile['region']}")
            
            if info_list:
                user_context_str = "\n".join(info_list)
            else:
                user_context_str = "(ì‚¬ìš©ì ì •ë³´ ì—†ìŒ)"
        else:
            user_context_str = "(ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ì‚¬ìš©ì ë˜ëŠ” ì •ë³´ ì—†ìŒ)"

        formatted_history = []
        user_msg = None
        for turn in chat_history_list:
            if turn.get("sender") == "user":
                user_msg = turn.get("text", "")
            elif turn.get("sender") == "bot" and user_msg is not None:
                formatted_history.append((user_msg, turn.get("text", "")))
                user_msg = None 

        # LLM í˜¸ì¶œ
        response = qa_chain.invoke({
            "question": user_message, 
            "chat_history": formatted_history,
            "user_context_prompt": user_context_str 
        })
        
        bot_reply = response['answer']
        
        # ì¶œì²˜ í‘œì‹œ ë¡œì§
        source_doc = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        if response.get('source_documents'):
            metadata = response['source_documents'][0].metadata
            source_doc = metadata.get('title') or metadata.get('policy_name', 'ì¶œì²˜ ì •ë³´ ì—†ìŒ')

        print(f"Gemini ë‹µë³€: {bot_reply}")
        print(f"ë‹µë³€ ê·¼ê±°: {source_doc}")

        return {"answer": bot_reply, "source": source_doc}

    except Exception as e:
        print(f"ğŸš¨ RAG ì„œë²„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"answer": "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": None}


# --- 5. API ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    print(f"Python RAG API ì„œë²„ë¥¼ 8001ë²ˆ í¬íŠ¸ì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤ (http://localhost:8001)")
    run(app, host="0.0.0.0", port=8001)